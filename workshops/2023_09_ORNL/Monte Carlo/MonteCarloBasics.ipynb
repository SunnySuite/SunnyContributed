{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Basic Monte Carlo Methods\n",
    "### Ying Wai Li and Matthew Wilson (Los Alamos National Laboratory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Equilibrium statistical mechanics at a glance\n",
    "\n",
    "* A physical system has many ways to arrange itself $\\rightarrow$ a configuration/state $\\boldsymbol{x}$\n",
    "* Different configurations correspond to different total energies $E$ $(=E(\\boldsymbol{x}))$.\n",
    "\n",
    "**Canonical ensemble**: system with a heat bath at fixed temperature ($T$), number of particles ($N$), volume ($V$) &rarr; (\"$NVT$\" ensemble)\n",
    "\n",
    "**Canonical/Boltzmann distribution:**\n",
    "\n",
    "Given temperature $T$, what is the probability of finding the system at state $\\boldsymbol{x}$?\n",
    "$$ P(\\boldsymbol{x},T) = \\frac{1}{Z(T)} e^{-E(\\boldsymbol{x})/kT} $$\n",
    "\n",
    "How do the total energies $E$ of the system distribute?\n",
    "$$ P(E,T) = \\frac{1}{Z(T)} g(E) e^{-E/kT}$$\n",
    "\n",
    "where $Z(T)$ is the partition function:\n",
    "$$Z(T) = \\sum_{\\boldsymbol{x}} e^{-E(\\boldsymbol{x})/kT} = \\sum_{E} g(E) e^{-E(\\boldsymbol{x})/kT}$$\n",
    "\n",
    "$k$: Boltzmann constant\\\n",
    "$g(E)$: \"density of states\" that counts the energy degeneracy\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Figures/Boltzmann_distribution.png\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "**Internal energy $U(T)$ of the system at $T$:**\n",
    "$$U(T) = \\langle E \\rangle _T = \\sum_E E \\cdot P(E,T) = \\frac{1}{Z(T)} \\sum_E E g(E) e^{-E/kT} $$ \n",
    "\n",
    "where the angle brackets $\\langle ... \\rangle$ denote the ensemble average over the canonical ensemble.\n",
    "\n",
    "**Heat capacity $C_V(T)$:**\n",
    "$$C_V(T) = \\frac{dU}{dT}$$\n",
    "\n",
    "#### Connection between statistical mechanics and Monte Carlo (MC) simulations\n",
    "\n",
    "* Total number of admissable states of a system can be more than astronomical\\\n",
    "(A 17 $\\times$ 17 2D Ising model already has more states than the number of atoms in the universe = $10^{82}$!) \n",
    "* Monte Carlo (MC) sampling is a tool for sampling/selecting states according to a **desired probability distribution**\n",
    "* Monte Carlo is best at studying **finite-temperature** equilibrium properties, phase transitions, and mapping phase diagrams\n",
    "* For ground states optimization, choose your MC tools carefully! You probably want to use optimization techniques such as simulated annealing or parallel tempering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Metropolis sampling\n",
    "\n",
    "* Generates a collection of states (energies) that obeys the canonical distribution at temperature $T$\n",
    "* Metropolis sampling [J. Chem. Phys. **21**, 1087-1092 (1953)] is perhaps the most widely used importance sampling technique\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Figures/Metropolis.png\" width=\"800\" />\n",
    "</p>\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "0. Prerequisites: choose a temperature $T$, number of MC steps to be performed, **equilibrate the system** (to be covered in MC practical).\n",
    "1. Generate/propose a new configuration\n",
    "2. Calculate the change in energy, $\\Delta E = E(\\boldsymbol{x'}) - E(\\boldsymbol{x})$\n",
    "3. Accept/reject with probability:\n",
    "    $$ p_{acc}(\\boldsymbol{x}\\rightarrow\\boldsymbol{x'}) = \\min\\left(1, \\frac{P(E(\\boldsymbol{x'}), T)}{P(E(\\boldsymbol{x}), T)}\\right) = \\min\\left(1, e^{-\\Delta E/(k_BT)}\\right). $$\n",
    "\n",
    "    What is *really* happening at this step in a computer program?\n",
    "    * If $e^{-\\Delta E/(k_BT)} > 1$, accept the proposal $\\boldsymbol{x'}$ right away\n",
    "    * Otherwise, generate a random number $r \\in [0,1)$\n",
    "    * If $r > e^{-\\Delta E/(k_BT)}$, accept the proposal\n",
    "    * Otherwise, reject the proposal. **Keep the old state $\\boldsymbol{x}$ and count it again.**\n",
    "\n",
    "4. Calculate and accumulate all physical observables of interest (e.g. energy $E$, magnetization $M$, etc.)\n",
    "5. Repeat steps 1-4 until a desired number of MC steps have been performed\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Figures/MarkovChain.png\" width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "#### Data post-processsing: calculate ensemble averages of physical observables\n",
    "\n",
    "Assuming $N$ **statistically independent** samples are collected from the Metropolis simulation at temperature $T$, one can calculate different physical observables as follows:\n",
    "\n",
    "Internal energy: \n",
    "$$U(T) = \\langle E \\rangle _T = \\frac{1}{N} \\sum_i^N E_i $$\n",
    "\n",
    "Magnetization:\n",
    "$$M(T) = \\langle M \\rangle _T = \\frac{1}{N} \\sum_i^N M_i $$\n",
    "\n",
    "For any observable $\\mathcal{O}$ in general:\n",
    "$$ \\langle \\mathcal{O} \\rangle _T = \\frac{1}{N} \\sum_i^N \\mathcal{O}_i $$ \n",
    "\n",
    "Heat capacity:\n",
    "$$C_V(T) = \\frac{\\langle E^2 \\rangle - \\langle E \\rangle^2}{kT^2}$$\n",
    "\n",
    "Magnetic susceptability:\n",
    "$$\\chi(T) = \\frac{\\langle M^2 \\rangle - \\langle M \\rangle^2}{kT}$$\n",
    "\n",
    "... and all other order parameters you would like to calculate!\n",
    "\n",
    "*Questions:*\n",
    "1. The Boltzmann distribution term $P(E,T)$ disappears in the equations. Why?\n",
    "2. How to generate statistically independent samples?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Statistical analysis\n",
    "\n",
    "#### 3.1 Basics\n",
    "Given a set of $N$ **statistically independent** measurements of observable $\\mathcal{O} = \\{ \\mathcal{O}_1, \\mathcal{O}_2, ...,  \\mathcal{O}_i, ..., \\mathcal{O}_N \\}$,\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Figures/dataset.png\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "Mean: \n",
    "$$\\overline{\\mathcal{O}} = \\left< \\mathcal{O} \\right> = \\frac{1}{N}\\sum_i^N \\mathcal{O}_i$$\n",
    "\n",
    "Variance: \n",
    "$$\\sigma(\\mathcal{O})^2 = \\frac{1}{N} \\sum_i^N (\\mathcal{O}_i - \\overline{\\mathcal{O}})^2 = \\left< \\mathcal{O}^2 \\right> - \\left< \\mathcal{O} \\right>^2 $$\n",
    "[$\\sigma(\\mathcal{O})$: standard deviation]\n",
    "\n",
    "Variance of the mean: \n",
    "$$ \\sigma(\\overline{\\mathcal{O}})^2 = \\frac{\\sigma(\\mathcal{O})^2}{N-1}$$\n",
    "\n",
    "#### 3.2 Binning analysis: \n",
    "\n",
    "What if we would like to estimate the errors for a quantity that is not \"measured\" directly at each Monte Carlo step (e.g. the heat capacity $C_V$ is calculated from the energy fluctuations measured throughput the simulation)?\n",
    "\n",
    "**Blocking:**\n",
    "* Divide the whole data set into $N_b$ blocks, each has $N / N_b$ measurements\n",
    "* Calculate the observable of interest (e.g. $C_V$) for each block $j$, denoted by $\\mathcal{O}_{b,j}$\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Figures/blocking.png\" width=\"700\"/>\n",
    "</p>\n",
    "\n",
    "* Mean and variance of the quantity can be calculated like before: \n",
    "$$ \\overline{\\mathcal{O}_b} = \\frac{1}{N_b}\\sum_j^{N_b} \\mathcal{O}_{b,j} $$\n",
    "$$ \\sigma(\\mathcal{O}_b)^2 = \\frac{1}{N_b} \\sum_j^{N_b} (\\mathcal{O}_{b,j} - \\overline{\\mathcal{O}_b})^2 $$\n",
    "\n",
    "**Bootstrapping/resampling:**\n",
    "* Construct $M$ sets of data, each has $N$ measurements sampled from the original data set $\\mathcal{O}$. Denote each set as $\\mathcal{O}_m$ where $m = 1, 2, ..., M$. \n",
    "* Possible to have a measurement appearing multiple times in the resampled sets.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Figures/bootstrapping.png\" width=\"700\"/>\n",
    "</p>\n",
    "\n",
    "* Mean of the measurements for each set:\n",
    "$$\\overline{\\mathcal{O_m}} = \\frac{1}{N}\\sum_i^N \\mathcal{O}_{m,i}$$\n",
    "* Mean of the means to be the estimator of that observable:\n",
    "$$\\overline{\\mathcal{O}} = \\frac{1}{M}\\sum_j^M \\overline{\\mathcal{O}_m} $$\n",
    "* Error estimation:\n",
    "$$\\sigma(\\overline{\\mathcal{O}})^2 = \\overline{\\mathcal{O}^2} - \\overline{\\mathcal{O}}^2 $$\n",
    "\n",
    "\n",
    "**Jackknife analysis:**\n",
    "* The \"leave-one-out\" method; similar to cross validation in machine learning\n",
    "* Construct a new, reduced data set by leaving out one measurement from the full data set $\\mathcal{O}$\n",
    "* There are $N$ options to do so, this yields $N$ new data sets each with $N-1$ measurements.\n",
    "* Mean and variance can be calculated similarly. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Figures/jackknife.png\" width=\"700\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Autocorrelation function and correlation time\n",
    "\n",
    "The correlation time gives us an idea of the typical time scale for a state to transition to another significantly different (uncorrelated) state. (Think about the half-life of radioactive decay.) This is useful for us to obtain statistically independent samples.\n",
    "\n",
    "#### 4.1 Definitions\n",
    "\n",
    "**Time-displaced autocorrelation for an observable $\\mathcal{O}$:**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{A}(t) &= \\int dt' \\left[ \\mathcal{O}(t') - \\langle \\mathcal{O} \\rangle \\right] \\left[ \\mathcal{O}(t'+ t) - \\langle \\mathcal{O} \\rangle \\right] \\\\\n",
    "               &= \\int dt' \\left[ \\mathcal{O}(t')\\mathcal{O}(t'+ t) - \\langle \\mathcal{O} \\rangle^2 \\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Autocorrelation function (discretized and normalized with the observables's fluctuation):**\n",
    "\n",
    " $$ \\mathcal{A}(t) = \\frac{\\left<\\mathcal{O}_i\\mathcal{O}_{i+t}\\right> - \\left<\\mathcal{O}_i\\right>^2}{\\left<\\mathcal{O}_i^2\\right> - \\left<\\mathcal{O}_i\\right>^2} $$\n",
    "\n",
    "[TODO: will add a picture illustration]\n",
    "\n",
    "For example, to calculate $\\left<\\mathcal{O}_i\\mathcal{O}_{i+2}\\right>$:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Figures/correlation_function.png\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "**Correlation time**\n",
    "\n",
    "* The autocorrelation function $\\mathcal{A}(t)$ is expected to decay exponentially at long times, hence $\\mathcal{A}(t) \\sim e^{-t/\\tau}$ for $t \\rightarrow \\infty$\n",
    "* $\\tau$ is defined as the correlation time\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Figures/correlation_time.png\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "**Integrated correlation time**\n",
    "\n",
    "If, the variance of the mean is proven to be: \n",
    "    $$ \\sigma(\\overline{\\mathcal{O}})^2 = \\frac{\\sigma(\\mathcal{O})^2}{N} \\left[ 1 + 2 \\sum_t^{N-1} \\left( 1 - \\frac{t}{N} \\right) a(t) \\right], $$ \n",
    "\n",
    "where $a(t) = \\mathcal{A}(t) / \\mathcal{A}(0)$.\n",
    "\n",
    "The integrated correlation time is defined as (in discrete form):\n",
    "\n",
    "$$\\tau_{\\rm int} =  1 + 2 \\sum_t^{N-1} \\left( 1 - \\frac{t}{N} \\right) a(t)$$\n",
    "\n",
    "In the long time limit of $N \\rightarrow \\infty$,\n",
    "\n",
    "$$\\tau_{\\rm int} = 1 + 2 \\sum_t^{\\infty} a(t) $$\n",
    "\n",
    "\n",
    "#### 4.2 Implementation details\n",
    "\n",
    "* The calculation of autocorrelation function has an $\\mathcal{O}(N^2)$ computational complexity, where $N$ is the number of samples. ($\\mathcal{O}$ as for the [Big-O notation](https://en.wikipedia.org/wiki/Big_O_notation), not to be confused with the observable symbol $\\mathcal{O}$.) \n",
    "* Assume that the samples are a series of $N$ evenly-spaced measurements, a trick to compute autocorrelation function is by using fast Fourier transform (FFT) to calculate the Fourier transform $\\tilde{\\mathcal{A}}(\\omega)$ of the autocorrelation and then inverting the Fourier transform to get $\\mathcal{A}(t)$ back. This has an $\\mathcal{O}(N \\log N)$ computational complexity.\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\tilde{\\mathcal{A}}(\\omega) &= \\int dt e^{-i \\omega t} \\mathcal{A}(t) \\\\\n",
    "                                &= \\int dt e^{-i \\omega t} \\int dt' \\left[ \\mathcal{O}(t') - \\langle \\mathcal{O} \\rangle \\right] \\left[ \\mathcal{O}(t'+ t) - \\langle \\mathcal{O} \\rangle \\right] \\\\\n",
    "                                &= \\int dt \\int dt' e^{i \\omega t'} \\left[ \\mathcal{O}(t') - \\langle \\mathcal{O} \\rangle \\right]  e^{-i \\omega (t'+t)} \\left[ \\mathcal{O}(t'+ t) - \\langle \\mathcal{O} \\rangle \\right] \\\\\n",
    "                                &= \\tilde{\\mathcal{O}'}(\\omega) \\tilde{\\mathcal{O}'}(-\\omega) \\\\\n",
    "                                &= \\left| \\tilde{\\mathcal{O}'}(\\omega) \\right|^2,\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "    where $\\tilde{\\mathcal{O}'}(\\omega)$ is the Fourier transform of $\\mathcal{O}'(t) \\equiv \\mathcal{O}(t) - \\langle \\mathcal{O} \\rangle$.\n",
    "\n",
    "* So we can first calculate $\\tilde{\\mathcal{O}'}(\\omega)$ (the Fourier transform of the \"time series\" of observable $\\mathcal{O}(t)$ shifted by a constant $\\langle \\mathcal{O} \\rangle$), get $\\tilde{\\mathcal{A}}(\\omega)$, then calculate $\\mathcal{A}(t)$ by:\n",
    "\n",
    "$$\n",
    "\\mathcal{A}(t) = \\int d\\omega e^{i \\omega t} \\tilde{\\mathcal{A}}(\\omega).\n",
    "$$\n",
    "\n",
    "#### 4.3 A few notes on practicals:\n",
    "* The autocorrelation function has different \"stages\"\n",
    "    - (I): At short time $t$, combination of multiple correlation times at play\n",
    "    - (II): rather exponential, one of the characteristic correlation time dominates\n",
    "    - (III): At large time $t$, statistical noise dominates\n",
    "    \n",
    "    $\\rightarrow$ choose the region carefully when fitting the correlation time\n",
    "<p align=\"center\">\n",
    "  <img src=\"Figures/correlation_stages.png\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "* Different physical quantities have different correlation times (e.g. $E$ vs $M$)\n",
    "* The calculation of integrated correlation time is often more stable than the (exponential) correlation time\n",
    "* Samples drawn at intervals of $2 \\tau$ present reasonable statistical independence\n",
    "* It can be proven that the integrated correlation time $\\tau_{\\rm int}$ is roughly $2 \\tau$\n",
    "* Correlation time is different at different temperature $T$. It can be extremely long around transition temperature (e.g. critical slowing down around second order phase transition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Finite size effects\n",
    "\n",
    "1. Different system sizes you put in the MC simulations will give you qualitatively different results.\n",
    "\n",
    "    e.g. Parallel tempering simulation on 2D ferromagnetic Ising model of different system sizes\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Figures/ising_finite_size.png\" width=\"900\"/>\n",
    "</p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
